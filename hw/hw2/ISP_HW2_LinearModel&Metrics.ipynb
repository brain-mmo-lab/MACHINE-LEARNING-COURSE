{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c3b1c0-fa60-42af-8bbd-5efcc0368121",
   "metadata": {},
   "source": [
    "# Домашнее задание №2. Линейная регрессия. Расширение признакового пространства. Метрики качества. Валидация.\n",
    "\n",
    "## Ф.И.О: _________\n",
    "\n",
    "### Описание.\n",
    "\n",
    "Домашнее задание состоит из 2-х частей:\n",
    "  - теоретическая часть\n",
    "  - практическая часть\n",
    "    - реализация модуля линейной регрессии\n",
    "    -  эксперименты\n",
    "\n",
    "На проверку требуется отправить zip архив, который будет содержать следующие файлы:\n",
    "  - модуль ``modules`` с реализованными классами\n",
    "  - заполненный блокнот (теоретическая часть + эксперименты) в формате .ipybn \n",
    "  - заполненный блокнот в формате .html (File -> Save and Export Notebook As -> HTML -> ...)\n",
    "\n",
    "-------------------------\n",
    "\n",
    "## Теоретическая часть. (4 points)\n",
    "\n",
    "**№1.** (1 point) Найдите субдифференциалы для следующих функций:\n",
    "- $f(x) = \\max(0, 1 − ax), \\ \\ a \\ — \\ const$,  во всех точках\n",
    "\n",
    "- $f(x) = \\sin x, \\ x \\in [0; \\frac{3}{2} \\pi]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec017831-1f51-4bbf-a01f-935d2415d0ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71e7a07f-98ac-4bf0-95c9-e49e314998f5",
   "metadata": {},
   "source": [
    "**№2.** (1 point) Подробнее ознакомьтесь с материалом по ROC-AUC по [ссылке](https://alexanderdyakonov.wordpress.com/2017/07/28/auc-roc-%D0%BF%D0%BB%D0%BE%D1%89%D0%B0%D0%B4%D1%8C-%D0%BF%D0%BE%D0%B4-%D0%BA%D1%80%D0%B8%D0%B2%D0%BE%D0%B9-%D0%BE%D1%88%D0%B8%D0%B1%D0%BE%D0%BA/) и следующую решите задачу:\n",
    "  - Пусть на ответах алгоритма $m$ (принимающих значения от $0$ до $1$) задано распределение объектов класса $1$ (доля объектов класса $1$ в зависимости от ответа алгоритма) следующим образом:\n",
    "    - $\\mathbb{P} (m \\in [a, b] \\ | \\ y = 1) = \\int_a^b p(z)dz$\n",
    "\n",
    "  - Распределение объектов класса 0 задаётся так:\n",
    "    - $\\mathbb{P} (m \\in [a, b] \\ | \\ y = 0) = \\int_a^b (2 − p(z))dz$\n",
    "\n",
    "где $p(z) = −1.5z^2 + 3z$\n",
    "\n",
    "Найдите вероятностные оценки на величины TPR, FPR и ROC-AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cacb732-a068-4754-8985-7534155d63e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "659f02aa-0acd-4292-97e4-b1ce927d7e11",
   "metadata": {},
   "source": [
    "**№3.** (1 point) Пусть, $a = a(x)$ ответ алгоритма. На сколько может уменьшиться ROC-AUC при использовании функции $\\min(a, 0.5)$ над оценками алгоритма?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcbf6f5-4ddd-467a-8ba8-813db36cd47c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "214443e4-867c-4f2a-8252-bd66c6473f32",
   "metadata": {},
   "source": [
    "**№4.** (1 point) Докажите, что ROC-AUC случайного классификатора равана $0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655ba6c-fcda-47e2-880d-c420d94b59b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16365f21-7017-40ef-a541-4a75f5e86ceb",
   "metadata": {},
   "source": [
    "------------------------\n",
    "\n",
    "## Практическая часть. (16 points)\n",
    "\n",
    "Данная часть задания направлена на ознакомление с линейными моделями и градиентными методами обучения. В задании необходимо:\n",
    "1. Написать на языке Python собственную реализацию модели линейной регрессии с произвольной функцией потерь и реализацию функции и градиента функции потерь для линейной регрессии. Реализации можно частично проверить через юнит-тесты (запускаются командой ``pytest tests.py``).\n",
    "2. Провести описанные ниже эксперименты с модельными данными и приложенным датасетом.\n",
    "3. Написать отчёт о проделанной работе (в формате jupyter notebook).\n",
    "\n",
    "### Реализация алгоритмов. (3 points)\n",
    "\n",
    "Везде выборкой объектов будем понимать ``numpy.ndarray`` размера $N \\times D$ или разреженную матрицу ``scipy.sparse.csr_matrix`` того же размера, под ответами для объектов выборки будем понимать ``numpy.ndarray`` размера $N$ , где $N$ — количество объектов в выборке, $D$ — размер признакового пространства. Подрузамевается, что первый столбец выборки объектов соответствует признаку для смещения и равен единице.\n",
    "\n",
    "- **``losses.py``** (1 point)\n",
    "  - класс в этом модуле задаёт конкретную функцию потерь, которую можно использовать для обучения линейной модели. Обратите внимание на то, что подсчёт всех функций может быть полностью векторизован (т.е. их можно реализовать без циклов). Предложенная в задании функция потерь должна поддерживать использование $l2$-регуляризации. Обратите внимание, что признак для смещения **не** должен учитываться в регуляризаторе.\n",
    "  - Класс должен поддерживать как плотные матрицы (``numpy.ndarray``), так и разреженные матрицы (``scipy.sparse.csr_matrix``). Класс ``LinearLoss`` наследуется от абстрактного класса BaseLoss и реализует два метода: ``func`` и ``grad``.\n",
    "    - ``func(self, X, y, w)`` — вычисление значения функции потерь на матрице признаков $X$, векторе ответов $y$ с вектором весов $w$.\n",
    "    - ``grad(self, X, y, w)`` — вычисление значения градиента функции потерь на матрице признаков $X$, векторе ответов $y$ с вектором весов $w$.\n",
    "  - У обоих методов одинаковые аргменты:\n",
    "     - $X$ - выборка объектов\n",
    "     - $y$ - вектор ответов\n",
    "     - $w$ - вектор коэффициентов модели ``numpy.ndarray``.\n",
    "\n",
    "В данном задании предлагается реализовать следующую функцию потерь:\n",
    "\n",
    "$$L(X,y,w, \\lambda) = \\dfrac{1}{N} \\| Xw - y \\|_2^2 + \\lambda \\|w\\|_2^2$$\n",
    "\n",
    "- **``linear_model.py``** (2 points)\n",
    "  - модуль с реализацией линейной модели, поддерживающей обучение через полный и стохастический градиентные спуски. Линейная модель должна задаваться в классе LinearModel. Параметр $\\eta_k > 0$ — темп обучения (learning rate) для градиентного спуска, где $k$ — номер эпохи, должен параметризовываться формулой: $\\eta_k = \\dfrac{\\alpha}{k^{\\beta}}, \\ где \\ \\ \\alpha, \\beta \\ — \\ \\ заданные \\ константы, \\ \\ k \\ — \\ \\ номер \\ итерации$\n",
    "\n",
    "  - Описание методов класса:\n",
    "    - ``__init__`` — конструктор (инициализатор) класса с параметрами:\n",
    "      - ``loss_function`` — функция потерь, заданная классом, наследованным от ``BaseLoss``\n",
    "      - ``batch_size`` — размер подвыборки, по которой считается градиент, если ``None``, то необходимо использовать полный градиент\n",
    "      - ``step_alpha`` — параметр выбора шага градиентного спуска\n",
    "      - ``step_beta`` — параметр выбора шага градиентного спуска\n",
    "      - ``tolerance`` — точность, по достижении которой, необходимо прекратить оптимизацию\n",
    "      - ``max_iter`` — максимальное число итераций \n",
    "    - ``fit(self, X, y, w_0=None, trace=False)`` — обучение линейной модели:\n",
    "      - ``X`` - выборка объектов\n",
    "      - ``y`` - вектор ответов\n",
    "      - ``w_0`` - начальное приближение вектора весов, если ``None``, то необходимо инициализировать внутри метода\n",
    "      - ``trace`` - индикатор, нужно ли возвращать информацию об обучении\n",
    "        - Если ``trace is True``, то метод должен вернуть словарь ``history``, содержащий информацию о поведении метода оптимизации во время обучения. Длина словаря ``history`` — количество эпох. Элементы словаря в случае полного градиентного спуска:\n",
    "          - ``history[’time’]`` — содержит время потраченное на обучение каждой эпохи\n",
    "          - ``history[’func’]`` — содержит значения функционала на обучающей выборке на каждой эпохе\n",
    "          - ``history[’func_val’]`` — содержит значения функционала на валидационной выборке на каждой эпохе\n",
    "        - Обратите внимание, что ``trace is True`` сильно замедляет обучение методов, т.к. требует в конце эпохи подсчитывать значение функции. Не используйте его ни в каких экспериментах, кроме экспериментов, где необходимо исследовать поведение функции в зависимости от гиперпараметров.\n",
    "        - Нет необходимости проводить честное семплирование для каждого батча в методе стохасического градиентного спуска. Вместо этого предлагается в начале одной эпохи сгенерировать случайную перестановку индексов объектов, а затем последовательно выбирать объекты для нового батча из элементов этой перестановки.\n",
    "    - ``predict(self, X)`` — получение предсказаний модели\n",
    "      - ``X`` - выборка объектов\n",
    "      - -> Метод должен вернуть ``numpy.ndarray`` такого же размера, как и первая размерность матрицы ``X``.\n",
    "    - ``get_objective(self, X, y)`` — получение предсказаний модели\n",
    "      - ``X`` - выборка объектов\n",
    "      - ``y`` - вектор ответов\n",
    "      - -> Функция должна вернуть вещественное число..\n",
    "    - ``get_weights(self)`` — получить вектор линейных коэффициентов модели\n",
    "\n",
    "--------------------------\n",
    "\n",
    "### Эксперименты. (13 points)\n",
    "\n",
    "Данные для этого задания находятся в файле ``hw2_data.csv``. Данные состоят из двух колонок: ``text`` и ``y``. Текст представляет собой комментарии пользователей. А целевая переменная, колонка ``y``, отражает степень токсичности комментария, которую вам необходимо будет предсказать.\n",
    "\n",
    "**1.** (2 points)\n",
    "\n",
    "*1.1* Произведите предварительную обработку текста. Приведите все тексты к нижнему регистру. Замените в тексте все символы, не являющиеся буквами и цифрами, на пробелы. Примените алгоритм [лемматизации](https://ru.wikipedia.org/wiki/%D0%9B%D0%B5%D0%BC%D0%BC%D0%B0%D1%82%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F) (например, [WordNetLemmatizer](https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html?highlight=wordnet) из библотеки [nltk](https://www.nltk.org/index.html)) к коллекции. Удалите из текста стоп-слова (например, используя список стоп-слов из nltk).\n",
    "\n",
    "Замечание. Полезные функции: ``str.lower``, ``str.split``, ``str.isalnum``, ``re.sub``, ``re.split``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf322658-343e-465b-bdaa-a739e75571d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46fe4a01-d4dd-41af-9dcc-8a14fac12e4d",
   "metadata": {},
   "source": [
    "*1.2.* Разделите данные на обучение, валидацию и тест. Для теста выберете $30\\%$ __случайных__ объектов из датасета. Оставшиеся данные разбейте в соотношении $70/30$ (обучение/валидация). Рекомендуется использовать функцию ``sklearn.model_selection.train_test_split`` c параметром ``random_state=42``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e84833-9cc6-4025-94ec-7a7c7e3f3856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bc361fe-61dc-4615-b1e4-51d17248abd1",
   "metadata": {},
   "source": [
    "*1.3.* Преобразуйте текст в разреженную матрицу ``scipy.sparse.csr_matrix``, где значение $x$ в позиции $(i, j)$ сответствует [$tf-idf$](https://ru.wikipedia.org/wiki/TF-IDF) характеристке $j$-го слова в $i$-ом документе. Рекомендуется использовать конструктор [``sklearn.feature_extraction.text.TfidfVectorizer``](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Добавьте в данные единичный столбец на нулевой позиции.\n",
    "\n",
    "Замечание 1. У ``TfidfVectorizer`` есть несколько методов для работы, используйте ``fit_transform`` и ``fit`` для обучающей выборки, используйте ``transform`` для тестовой.\n",
    "\n",
    "Замечание 2. Используйте параметр ``min_df``, чтобы уменьшить размерность данных и ускорить проведение экспериментов. Рекомендуется использовать ``min_df`` не меньше 5.\n",
    "\n",
    "Замечание 3. Для добавления единичного столбца, можно воспользоваться следующей инструкцией: ``from scipy.sparse import hstack, csr_matrix\n",
    "X = csr_matrix(hstack([csr_matrix(np.ones((X.shape[0], 1))), X]))``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d8b34-f899-47f8-b4f7-bb094da34a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed8a2ba1-b7fc-469f-91ff-978d07092b85",
   "metadata": {},
   "source": [
    "**2.** (3 points) В спецификации предлагается использовать следующую формулу для выбора темпа обучения $\\gamma_k$:\n",
    "\n",
    "$$\\gamma_k = \\dfrac{\\alpha}{k^{\\beta}}, \\ где \\ \\ \\alpha, \\beta \\ — \\ заданные \\ константы, \\ \\ k \\ — \\ номер \\ итерации$$\n",
    "\n",
    "   - Исследуйте поведение градиентного спуска для задачи линейной регрессии в зависимости от следующих параметров:\n",
    "        - параметр темпа обучения ``step_alpha``\n",
    "        - параметр темпа обучения ``step_beta``\n",
    "\n",
    "   - Исследование поведения подразумевает анализ следующих зависимостей на обучающей и валидационной выборках:\n",
    "        - зависимость значения функции потерь от реального времени работы метода\n",
    "        - зависимость значения функции потерь от эпохи метода\n",
    "        - значение метрики качества после обучения метода\n",
    "\n",
    "В качестве метрики качества здесь и далее предлагается использовать **MAE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aeb754-e0ff-463e-adb8-27a81068fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.linear_model import LinearModel\n",
    "from modules.losses import LinearLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0fc48-b348-45b9-bf72-bac83fd3ae1d",
   "metadata": {},
   "source": [
    "**5.** (3 points)\n",
    "- Исследуйте поведение стохастического градиентного спуска для задачи линейной регрессии в зависимости от следующих параметров:\n",
    "  - параметр темпа обучения ``step_alpha``\n",
    "  - параметр темпа обучения ``step_beta``\n",
    "  - размер подвыборки ``batch_size``\n",
    "\n",
    "Замечание. Обратите внимание, что в стохастическом случае необходимо строить зависимости метрик качества от эпохи метода. За одну эпоху через оптимизацию модели проходит $N$ объектов, где $N$ — длина обучающей выборки. Если вы реализуете семплирование согласно спецификации задания, то за одну эпоху каждый объект пройдёт через оптимизацию ровно один раз. В полном градиентном спуске одна эпоха метода соответствует одной итерации обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87e260-7e11-4573-ad67-b8b3963b7c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "743cd09b-fc13-43af-b106-f2ffa70d61ce",
   "metadata": {},
   "source": [
    "**6.** (2 point) Сравните поведение двух методов между собой, сделайте выводы. Сравните оптимальные ``step_alpha`` и ``step_beta`` для разных методов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea7527-5b84-47c3-814f-c5103d2c8b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c04eb009-7045-425d-acae-1f9a4c05c441",
   "metadata": {},
   "source": [
    "**7.** (1 point) Подберите по отложенной выборке коэффициент $l2$-регуляризации модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e39dc-ca9e-4826-baf7-82ecb5fea66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5557a84e-9377-4c11-9428-995700331a0d",
   "metadata": {},
   "source": [
    "**8.** (2 points) Выберите лучший алгоритм для тестовой выборки. Проанализируйте ошибки алгоритма. Проанализируйте и укажите общие черты объектов, на которых были допущены ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2803cc0-674e-427c-9123-c2cc59c346aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
